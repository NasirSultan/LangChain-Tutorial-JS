

# Using Memory in LLM with LangChain 



This  shows how to use memory with a language model using LangChain and Google's Gemini model.

---

**What This Project Does:**

* Connects to Google's Gemini Flash model through the LangChain library.
* Uses memory to store previous messages.
* Shows how a language model can remember and use earlier inputs in later answers.
* Helps you understand how memory works in AI tools.

---

**Main Concepts:**

1. **LLM (Large Language Model):**
   A powerful AI model that can understand and generate natural language.

2. **LangChain:**
   A tool that helps developers connect LLMs with prompts, memory, and workflows.

3. **Memory (BufferMemory):**
   Stores earlier messages so the LLM can "remember" past inputs while answering future ones.

4. **Prompt Template:**
   A format that tells the AI how to respond and what to remember.

---

**How It Works:**

* A system message defines the AI’s tone and behavior.
* When you provide inputs, the system keeps track of them.
* Later inputs can be answered using earlier information stored in memory.

---

**Why Learn About Memory in LLMs:**

* It shows how AI can handle multi-turn conversations or tasks.
* It helps you build smarter AI applications that can understand context.
* It’s useful in education, research, personal assistants, and automation systems.

